{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fbb67f",
   "metadata": {},
   "source": [
    "## PDF Text extraction & chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8693ef",
   "metadata": {},
   "source": [
    "### PDF to text\n",
    "\n",
    "PDFs contain rich formatting that needs to be extracted as clean text for AI processing. \n",
    "\n",
    "Modern libraries like `docling` can preserve document structure while converting to text format, making the text easier to process while maintaining semantic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7edd6c",
   "metadata": {},
   "source": [
    "Inspect a PDF converted with `docling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9dd1ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:30.559154Z",
     "iopub.status.busy": "2025-09-09T12:44:30.558868Z",
     "iopub.status.idle": "2025-09-09T12:44:30.563615Z",
     "shell.execute_reply": "2025-09-09T12:44:30.563112Z",
     "shell.execute_reply.started": "2025-09-09T12:44:30.559125Z"
    },
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1756203792603,
    "lastExecutedByKernel": "fe7640a5-8f98-4365-917b-a2fa71df5513",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "md_filepath = Path(\"data/parsed/howto-free-threading-python-parsed-text.md\")\nmd_txt = md_filepath.read_text()\nprint(md_txt[:2000])",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "md_filepath = Path(\"data/parsed/amazon-2025-08-8k-excerpts-parsed-text.md\")\n",
    "md_txt = md_filepath.read_text()\n",
    "print(md_txt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d0596",
   "metadata": {},
   "source": [
    "Tables preserve their formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843973f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:30.566716Z",
     "iopub.status.busy": "2025-09-09T12:44:30.566457Z",
     "iopub.status.idle": "2025-09-09T12:44:30.570131Z",
     "shell.execute_reply": "2025-09-09T12:44:30.569487Z",
     "shell.execute_reply.started": "2025-09-09T12:44:30.566697Z"
    }
   },
   "outputs": [],
   "source": [
    "table_example = md_txt.split(\"## Consolidated Statements of Operations\")[1]\n",
    "print(table_example[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b62b17",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Raw text from PDFs is often too long for AI models to process effectively. Chunking breaks documents into smaller, manageable pieces while preserving context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f956e",
   "metadata": {},
   "source": [
    "#### Chunk by text length\n",
    "\n",
    "Fixed-length chunks are simple but can break sentences or paragraphs mid-thought. This approach is fast and predictable, making it suitable for initial processing or when document structure is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6773b88",
   "metadata": {},
   "source": [
    "![images/chunking_why.png](images/chunking_why.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99973c13",
   "metadata": {},
   "source": [
    "Different chunking strategies serve different use cases. \n",
    "\n",
    "![images/chunking_methods.png](images/chunking_methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c3c26",
   "metadata": {},
   "source": [
    "Let's try a few options:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddfa414",
   "metadata": {},
   "source": [
    "#### Chunk by text length with overlap\n",
    "\n",
    "Overlapping chunks help maintain context across boundaries. \n",
    "\n",
    "When a concept spans multiple chunks, the overlap helps to capture it. This is especially important for maintaining semantic coherence in search and retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3329d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:30.580509Z",
     "iopub.status.busy": "2025-09-09T12:44:30.580339Z",
     "iopub.status.idle": "2025-09-09T12:44:30.584453Z",
     "shell.execute_reply": "2025-09-09T12:44:30.583723Z",
     "shell.execute_reply.started": "2025-09-09T12:44:30.580493Z"
    },
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1756203792650,
    "lastExecutedByKernel": "fe7640a5-8f98-4365-917b-a2fa71df5513",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def get_chunks_by_length_with_overlap(src_text: str, chunk_length: int = 500, overlap: int = 100) -> list[str]:\n    \"\"\"\n    Split text into chunks of approximately `chunk_length` characters.\n    \"\"\"\n    chunks = []\n    for i in range(0, len(src_text), chunk_length):\n        chunks.append(src_text[i:i + chunk_length + overlap])\n    return chunks"
   },
   "outputs": [],
   "source": [
    "def get_chunks_by_length_with_overlap(src_text: str, chunk_length: int = 500, overlap: int = 100) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks of approximately `chunk_length` characters.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(src_text), chunk_length):\n",
    "        chunks.append(src_text[i:i + chunk_length + overlap])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc39a6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:30.586122Z",
     "iopub.status.busy": "2025-09-09T12:44:30.585885Z",
     "iopub.status.idle": "2025-09-09T12:44:30.589576Z",
     "shell.execute_reply": "2025-09-09T12:44:30.589010Z",
     "shell.execute_reply.started": "2025-09-09T12:44:30.586106Z"
    },
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1756203792700,
    "lastExecutedByKernel": "fe7640a5-8f98-4365-917b-a2fa71df5513",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "chunks = get_chunks_by_length_with_overlap(md_txt)\ndisplay(chunks[:5])\nprint(len(chunks[0]))",
    "outputsMetadata": {
     "1": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# STUDENT TODO\n",
    "# Chunk `md_text_1` with `get_chunks_by_length_with_overlap`\n",
    "# Inspect the first 3 or so chunks\n",
    "# START_SOLUTION\n",
    "chunks = get_chunks_by_length_with_overlap(md_txt)\n",
    "\n",
    "for chunk in chunks[:3]:\n",
    "    print(f\"Chunk length: {len(chunk)}\\n\" + \"-\"*20)\n",
    "    print(chunk)\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694e768",
   "metadata": {},
   "source": [
    "#### Chunk using markers\n",
    "\n",
    "Using document markers (like headers) creates chunks that respect natural document boundaries. \n",
    "\n",
    "This approach preserves semantic structure and is ideal for documents with clear hierarchical organization like reports, manuals, or academic papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35daea7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:30.592389Z",
     "iopub.status.busy": "2025-09-09T12:44:30.592191Z",
     "iopub.status.idle": "2025-09-09T12:44:30.595835Z",
     "shell.execute_reply": "2025-09-09T12:44:30.595275Z",
     "shell.execute_reply.started": "2025-09-09T12:44:30.592372Z"
    },
    "executionCancelledAt": null,
    "executionTime": 46,
    "lastExecutedAt": 1756203792746,
    "lastExecutedByKernel": "fe7640a5-8f98-4365-917b-a2fa71df5513",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def get_chunks_using_markers(src_text: str) -> list[str]:\n    \"\"\"\n    Split the source text into chunks using markers.\n    \"\"\"\n    marker = \"\\n##\"\n\n    # Split by marker and reconstruct with markers (except first chunk)\n    parts = src_text.split(marker)\n    chunks = []\n\n    # Add first chunk if it exists and isn't empty\n    if parts[0].strip():\n        chunks.append(parts[0].strip())\n\n    # Add remaining chunks with markers reattached\n    for part in parts[1:]:\n        if part.strip():\n            chunks.append(marker + part.strip())\n\n    return chunks"
   },
   "outputs": [],
   "source": [
    "def get_chunks_using_markers(src_text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split the source text into chunks using markers.\n",
    "    \"\"\"\n",
    "    marker = \"\\n##\"\n",
    "\n",
    "    # Split by marker and reconstruct with markers (except first chunk)\n",
    "    parts = src_text.split(marker)\n",
    "    chunks = []\n",
    "\n",
    "    # Add first chunk if it exists and isn't empty\n",
    "    if parts[0].strip():\n",
    "        chunks.append(parts[0].strip())\n",
    "\n",
    "    # Add remaining chunks with markers reattached\n",
    "    for part in parts[1:]:\n",
    "        if part.strip():\n",
    "            chunks.append(marker + part.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd11080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:30.597485Z",
     "iopub.status.busy": "2025-09-09T12:44:30.597300Z",
     "iopub.status.idle": "2025-09-09T12:44:30.601621Z",
     "shell.execute_reply": "2025-09-09T12:44:30.601076Z",
     "shell.execute_reply.started": "2025-09-09T12:44:30.597469Z"
    },
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1756203792796,
    "lastExecutedByKernel": "fe7640a5-8f98-4365-917b-a2fa71df5513",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "md_file_1 = Path(\"data/parsed/howto-free-threading-python-parsed-text.md\")\nmd_text_1 = md_file_1.read_text(encoding=\"utf-8\")\nchunks = get_chunks_using_markers(md_text_1)\n\nfor chunk in chunks[:5]:\n    print(\"\\n\\nChunk: \" + \"=\" * 10 + f\"\\n{chunk}\")",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "md_file_1 = Path(\"data/parsed/amazon-2025-08-8k-excerpts-parsed-text.md\")\n",
    "md_text_1 = md_file_1.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# STUDENT TODO\n",
    "# Chunk `md_text_1` with `get_chunks_using_markers`\n",
    "# Inspect the first few chunks\n",
    "# START_SOLUTION\n",
    "chunks = get_chunks_using_markers(md_text_1)\n",
    "\n",
    "for chunk in chunks[:3]:\n",
    "    print(f\"Chunk length: {len(chunk)}\\n\" + \"-\"*20)\n",
    "    print(chunk)\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ce9f4",
   "metadata": {},
   "source": [
    "## Working with PDFs with images\n",
    "\n",
    "PDFs contain more than rich formatting - they have images!\n",
    "\n",
    "Run the cells below to convert PDF data to images (this should take about a minute). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da91ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:30.604098Z",
     "iopub.status.busy": "2025-09-09T12:44:30.603803Z",
     "iopub.status.idle": "2025-09-09T12:44:30.675240Z",
     "shell.execute_reply": "2025-09-09T12:44:30.674651Z",
     "shell.execute_reply.started": "2025-09-09T12:44:30.604080Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pymupdf\n",
    "except ImportError:\n",
    "    %pip install -Uqq pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0ea16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:30.676364Z",
     "iopub.status.busy": "2025-09-09T12:44:30.676069Z",
     "iopub.status.idle": "2025-09-09T12:44:35.428530Z",
     "shell.execute_reply": "2025-09-09T12:44:35.427944Z",
     "shell.execute_reply.started": "2025-09-09T12:44:30.676337Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python pdf_to_img.py WEF*.pdf\n",
    "echo \"Images extracted from AI report PDF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df7bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:35.429436Z",
     "iopub.status.busy": "2025-09-09T12:44:35.429209Z",
     "iopub.status.idle": "2025-09-09T12:44:36.592474Z",
     "shell.execute_reply": "2025-09-09T12:44:36.591689Z",
     "shell.execute_reply.started": "2025-09-09T12:44:35.429417Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "images = [\n",
    "    f\"data/imgs/WEF_Artificial_Intelligence_in_Financial_Services_2025_07_of_27.jpg\",\n",
    "    f\"data/imgs/WEF_Artificial_Intelligence_in_Financial_Services_2025_10_of_27.jpg\",\n",
    "    f\"data/imgs/WEF_Artificial_Intelligence_in_Financial_Services_2025_15_of_27.jpg\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 40))\n",
    "\n",
    "for i, img_path in enumerate(images):\n",
    "    img = Image.open(img_path)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2dc74d",
   "metadata": {},
   "source": [
    "### Approach 1 - Extract text and images separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf071709",
   "metadata": {},
   "source": [
    "Some libraries (like `docling`) can extract text and images from PDFs, and convert them into Markdown files.\n",
    "\n",
    "Here, we've pre-converted this PDF into markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bcf2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:36.600046Z",
     "iopub.status.busy": "2025-09-09T12:44:36.599677Z",
     "iopub.status.idle": "2025-09-09T12:44:36.604567Z",
     "shell.execute_reply": "2025-09-09T12:44:36.603946Z",
     "shell.execute_reply.started": "2025-09-09T12:44:36.600009Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "md_filepath = Path(\"data/parsed/WEF_Artificial_Intelligence_in_Financial_Services_2025-parsed-w-imgs.md\")\n",
    "md_txt = md_filepath.read_text()\n",
    "print(md_txt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b36ce8",
   "metadata": {},
   "source": [
    "#### Chunking text files with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba15854",
   "metadata": {},
   "source": [
    "More complex than just text, since we need to handle images as well.\n",
    "\n",
    "- Must include entire image string in the chunk\n",
    "- When vectorizing, optionally include base64 of image\n",
    "    - Your embedding model must be multimodal\n",
    "\n",
    "Chunking becomes more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4901b0",
   "metadata": {},
   "source": [
    "One method: try a specialized library like `chonkie` to handle this\n",
    "\n",
    "Chonkie offers a variety of chunking strategies:\n",
    "\n",
    "<img src=\"images/chonkie_methods.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde48702",
   "metadata": {},
   "source": [
    "There isn't going to be a \"one size fits all\" solution for chunking PDFs with images. But these libraries can help you get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213bafaa",
   "metadata": {},
   "source": [
    "Let's try a couple of different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d999613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:36.614315Z",
     "iopub.status.busy": "2025-09-09T12:44:36.614031Z",
     "iopub.status.idle": "2025-09-09T12:44:38.822696Z",
     "shell.execute_reply": "2025-09-09T12:44:38.821859Z",
     "shell.execute_reply.started": "2025-09-09T12:44:36.614282Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -Uqq \"chonkie[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130b019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:38.823893Z",
     "iopub.status.busy": "2025-09-09T12:44:38.823618Z",
     "iopub.status.idle": "2025-09-09T12:44:39.332703Z",
     "shell.execute_reply": "2025-09-09T12:44:39.332157Z",
     "shell.execute_reply.started": "2025-09-09T12:44:38.823837Z"
    }
   },
   "outputs": [],
   "source": [
    "from chonkie import RecursiveChunker\n",
    "\n",
    "# Initialize the recursive chunker to chunk Markdown\n",
    "chunker = RecursiveChunker.from_recipe(\"markdown\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea6140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:39.333828Z",
     "iopub.status.busy": "2025-09-09T12:44:39.333358Z",
     "iopub.status.idle": "2025-09-09T12:44:39.338008Z",
     "shell.execute_reply": "2025-09-09T12:44:39.337459Z",
     "shell.execute_reply.started": "2025-09-09T12:44:39.333799Z"
    }
   },
   "outputs": [],
   "source": [
    "chunk_texts = chunker.chunk(md_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b0306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:39.338743Z",
     "iopub.status.busy": "2025-09-09T12:44:39.338562Z",
     "iopub.status.idle": "2025-09-09T12:44:39.343671Z",
     "shell.execute_reply": "2025-09-09T12:44:39.343128Z",
     "shell.execute_reply.started": "2025-09-09T12:44:39.338726Z"
    }
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "for chunk in chunk_texts[:5]:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Chunk text:\")\n",
    "    wrapped_text = textwrap.fill(chunk.text[:500]+\"...\", width=80)\n",
    "    print(textwrap.indent(wrapped_text, \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4a5d3",
   "metadata": {},
   "source": [
    "Let's try a \"semantic\" chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01354461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:39.346554Z",
     "iopub.status.busy": "2025-09-09T12:44:39.346275Z",
     "iopub.status.idle": "2025-09-09T12:44:40.051317Z",
     "shell.execute_reply": "2025-09-09T12:44:40.050750Z",
     "shell.execute_reply.started": "2025-09-09T12:44:39.346527Z"
    }
   },
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=2048,                             # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ab886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:40.052238Z",
     "iopub.status.busy": "2025-09-09T12:44:40.051969Z",
     "iopub.status.idle": "2025-09-09T12:44:40.200040Z",
     "shell.execute_reply": "2025-09-09T12:44:40.199394Z",
     "shell.execute_reply.started": "2025-09-09T12:44:40.052218Z"
    }
   },
   "outputs": [],
   "source": [
    "chunk_texts = chunker.chunk(md_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06fb05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T12:44:40.200903Z",
     "iopub.status.busy": "2025-09-09T12:44:40.200681Z",
     "iopub.status.idle": "2025-09-09T12:44:40.205839Z",
     "shell.execute_reply": "2025-09-09T12:44:40.205138Z",
     "shell.execute_reply.started": "2025-09-09T12:44:40.200882Z"
    }
   },
   "outputs": [],
   "source": [
    "for chunk in chunk_texts[:5]:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Chunk text:\")\n",
    "    wrapped_text = textwrap.fill(chunk.text[:500]+\"...\", width=80)\n",
    "    print(textwrap.indent(wrapped_text, \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4326f",
   "metadata": {},
   "source": [
    "We get a relatively \"even\" distribution of chunks here. \n",
    "\n",
    "So let's continue on with this approach. "
   ]
  }
 ],
 "metadata": {
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
