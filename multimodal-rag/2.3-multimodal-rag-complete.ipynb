{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6861ab3-dffd-4da4-a819-b090b8ae756e",
   "metadata": {},
   "source": [
    "### Load chunks as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b54c46a-e478-46ea-9828-1505c9cfef26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T14:54:37.180356Z",
     "iopub.status.busy": "2025-09-04T14:54:37.180128Z",
     "iopub.status.idle": "2025-09-04T14:54:38.744294Z",
     "shell.execute_reply": "2025-09-04T14:54:38.743491Z",
     "shell.execute_reply.started": "2025-09-04T14:54:37.180335Z"
    }
   },
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "from pathlib import Path\n",
    "\n",
    "md_filepath = Path(\"data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs.md\")\n",
    "md_txt = md_filepath.read_text()\n",
    "\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=2048,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")\n",
    "chunk_texts = chunker.chunk(md_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2a8a4",
   "metadata": {},
   "source": [
    "### Set up Weaviate Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e6b22f3-07d4-4f7c-91c3-234ceb94511e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:19.043659Z",
     "iopub.status.busy": "2025-09-04T16:17:19.043354Z",
     "iopub.status.idle": "2025-09-04T16:17:19.059810Z",
     "shell.execute_reply": "2025-09-04T16:17:19.059098Z",
     "shell.execute_reply.started": "2025-09-04T16:17:19.043637Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers import update_creds\n",
    "\n",
    "AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_SESSION_TOKEN = update_creds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af250e-be00-421a-948c-3daeda7e3b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:20.600181Z",
     "iopub.status.busy": "2025-09-04T16:17:20.599677Z",
     "iopub.status.idle": "2025-09-04T16:17:20.686746Z",
     "shell.execute_reply": "2025-09-04T16:17:20.685956Z",
     "shell.execute_reply.started": "2025-09-04T16:17:20.600147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    WEAVIATE_IP,\n",
    "    headers = {\n",
    "        \"X-AWS-Access-Key\": AWS_ACCESS_KEY,\n",
    "        \"X-AWS-Secret-Key\": AWS_SECRET_KEY,\n",
    "        \"X-AWS-Session-Token\": AWS_SESSION_TOKEN,\n",
    "    }\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "842550b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:22.589947Z",
     "iopub.status.busy": "2025-09-04T16:17:22.589431Z",
     "iopub.status.idle": "2025-09-04T16:17:22.602223Z",
     "shell.execute_reply": "2025-09-04T16:17:22.601436Z",
     "shell.execute_reply.started": "2025-09-04T16:17:22.589904Z"
    }
   },
   "outputs": [],
   "source": [
    "client.collections.delete(\"Chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4e202bae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:23.571902Z",
     "iopub.status.busy": "2025-09-04T16:17:23.571605Z",
     "iopub.status.idle": "2025-09-04T16:17:23.744456Z",
     "shell.execute_reply": "2025-09-04T16:17:23.743658Z",
     "shell.execute_reply.started": "2025-09-04T16:17:23.571881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x7f2cf47225a0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weaviate.classes.config import Property, DataType, Configure, Tokenization\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Chunks\",\n",
    "    properties=[\n",
    "        Property(\n",
    "            name=\"document_title\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk_number\",\n",
    "            data_type=DataType.INT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"filename\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.FIELD\n",
    "        ),\n",
    "    ],\n",
    "    vector_config=[\n",
    "        Configure.Vectors.text2vec_aws(\n",
    "            name=\"default\",\n",
    "            source_properties=[\"document_title\", \"chunk\"],\n",
    "            region=\"us-west-2\",\n",
    "            service=\"bedrock\",\n",
    "            model=\"amazon.titan-embed-text-v2:0\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7c9e2f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:24.735497Z",
     "iopub.status.busy": "2025-09-04T16:17:24.735204Z",
     "iopub.status.idle": "2025-09-04T16:17:24.740484Z",
     "shell.execute_reply": "2025-09-04T16:17:24.739337Z",
     "shell.execute_reply.started": "2025-09-04T16:17:24.735474Z"
    }
   },
   "outputs": [],
   "source": [
    "chunks = client.collections.use(\"Chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f3bad",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dee53991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:25.946999Z",
     "iopub.status.busy": "2025-09-04T16:17:25.946725Z",
     "iopub.status.idle": "2025-09-04T16:17:27.760130Z",
     "shell.execute_reply": "2025-09-04T16:17:27.759319Z",
     "shell.execute_reply.started": "2025-09-04T16:17:25.946980Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:00, 20754.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with chunks.batch.fixed_size(batch_size=100) as batch:\n",
    "    for i, chunk_text in tqdm(enumerate(chunk_texts)):\n",
    "        obj = {\n",
    "            \"document_title\": \"HAI AI Index Report 2025\",\n",
    "            \"filename\": \"data/pdfs/hai_ai-index-report-2025_chapter2_excerpts.pdf\",\n",
    "            \"chunk\": chunk_text.text,\n",
    "            \"chunk_number\": i + 1,\n",
    "        }\n",
    "\n",
    "        # Add object to batch for import with (batch.add_object())\n",
    "        # BEGIN_SOLUTION\n",
    "        batch.add_object(\n",
    "            properties=obj\n",
    "        )\n",
    "        # END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed8bc9",
   "metadata": {},
   "source": [
    "### RAG queries\n",
    "\n",
    "How do we perform RAG in this scenario? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e0fed",
   "metadata": {},
   "source": [
    "This is a bit different, because we haven't embedded the images (or stored them in Weaviate).\n",
    "\n",
    "In this scenario, let's:\n",
    "\n",
    "- Retrieve text chunks\n",
    "- Get images referred to in the text\n",
    "- Convert the images to base64\n",
    "- Send (retrieved text + images + prompt) to LLM for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cae8dd66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:50.920122Z",
     "iopub.status.busy": "2025-09-04T16:17:50.919778Z",
     "iopub.status.idle": "2025-09-04T16:17:51.009371Z",
     "shell.execute_reply": "2025-09-04T16:17:51.008478Z",
     "shell.execute_reply.started": "2025-09-04T16:17:50.920098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "## Chapter 2: Technical Performance\n",
      "\n",
      "## RAG: Retrieval Augment Generation (RAG)\n",
      "\n",
      "An  increasingly  common  capability  being  tested  in  LLMs is retrieval-augmented generation (RAG). This approach integrates LLMs  with retrieval mechanisms  to enhance their  response  generation. The  model fi rst  retrieves  relevant information  from fi les  or  documents  and  then  generates  a response tailored to the user's query based on the retrieved content.  RAG  has  diverse  use  cases,  including  answering precise questions from large databases and addressing customer queries using information from company documents.\n",
      "\n",
      "models. 2024 also saw the release of numerous benchmarks for  evaluating  RAG  systems,  including  Ragnarok  (a  RAG arena battleground) and CRAG (Comprehensive RAG benchmark). Additionally, specialized RAG benchmarks, such as FinanceBench for fi nancial question answering, have been developed to address speci fi c use cases.\n",
      "\n",
      "## Berkeley Function Calling Leaderboard\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "In recent years, RAG has received increasing attention from researchers  and  companies.  For  example,  in  September 2024, Anthropic introduced Contextual Retrieval, a method that signi fi cantly enhances the retrieval capabilities of RAG\n",
      "\n",
      "The  Berkeley  Function  Calling  Leaderboard  evaluates  the ability  of  LLMs  to  accurately  call  functions  or  tools.  The evaluation suite includes over 2,000  question-functionanswer pairs across multiple programming languages (such as  Python,  Java,  JavaScript,  and  REST  API)  and  spans  a variety of testing domains (Figure 2.2.17).\n",
      "\n",
      "## Data composition on the Berkeley Function Calling Leaderboard\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "2.2 Language\n",
      "\n",
      "## MTEB: Massive Text Embedding Benchmark\n",
      "\n",
      "The  Massive Text  Embedding  Benchmark  (MTEB),  created by a team at Hugging Face and Cohere, was introduced in late 2022 to comprehensively evaluate how models perform on various embedding tasks. Embedding involves converting data,  such  as  words,  texts,  or  documents,  into  numerical vectors that capture rough semantic meanings and distance between vectors. Embedding is an essential component of RAG. During a RAG task, when users input a query, the model\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "2.9 Robotics and Automous Motion\n",
      "\n",
      "## Technical Innovations and New Benchmarks\n",
      "\n",
      "Over  the  past  year,  self-driving  technology  has  advanced signi fi cantly,  both  in  vehicle  capabilities  and  benchmarking methods.  In  October  2024,  Tesla  unveiled  the  Cybercab,  a two-passenger autonomous vehicle without a steering wheel or  pedals,  which  is  set  for  production  in  2026  at  a  price  of under  $30,000. Tesla  also  unveiled  the  Robovan,  an  electric autonomous van designed to transport up to 20 passengers. Meanwhile, Baidu's Apollo Go launched its latest-generation robotaxi, the RT6, across multiple cities in China (Figure 2.9.12). With a price tag of just $30,000 and a battery-swapping system, the  RT6  represents  a  major  step  toward  making  self-driving technology more cost-e ff ective and scalable. As costs continue to decline, the adoption of autonomous vehicles is expected to accelerate. Notable business partnerships have also advanced self-driving  tech...\n",
      "\n",
      "========================================\n",
      "\n",
      "Figure 2.8.1\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000007_3921e74bcd236b403bab6328e8a11a403024116e502a0d0bbf5264f43935b867.png)\n",
      "\n",
      "## 2.8 AI Agents Chapter 2: Technical Performance\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000008_33bf09ad90ca2e078ea69ec3e2e6beeea2fb2395ebd182a5b6e68c8d215648de.png)\n",
      "\n",
      "VAB presents a signi fi cant challenge for AI systems. ...\n",
      "\n",
      "========================================\n",
      "This is partly due to the  inherent  complexity  of  benchmarking  agentic tasks, which  are typically  more diverse,  dynamic,  and  variable  than  tasks  like  image  classi fi cation  or  answering language questions. As AI continues to evolve, it will become important to develop e ff ective methods to evaluate AI agents.\n",
      "\n",
      "## VisualAgentBench\n",
      "\n",
      "VisualAgentBench  (VAB),  launched  in  2024,  represents  a signi fi cant step forward in the evaluation of agentic AI. This benchmark re fl ects the growing multimodality of AI models and their increasing pro fi ciency in navigating both virtual and embodied environments. VAB addresses the need to assess agent  performance  in  diverse  settings  that  extend  beyond environments  reliant  solely  on  linguistic  commands.  VAB\n",
      "\n",
      "tests agents across three broad categories of tasks: embodied agents (operating  in  household  and  gaming  environments), GUI agents (interacting with mobile and web applications), and visual  design  agents  (suc...\n",
      "\n",
      "========================================\n",
      " According  to  the  benchmark's authors, these results reveal that current AI models are far from ready for direct deployment in agentic settings.\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000009_8670c86bc1f4a1d41097f5ec286999521dc5bbf65615ebf4fb76a6a03b679acd.png)\n",
      "\n",
      "Figure 2.8.2\n",
      "\n",
      "## RE-Bench\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "Source: Wijk et al., 2024 | Chart: 2025 AI Index report\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000012_90c7c87f35758c0ce8c4a95f97089830098242f6ae4df85ab58cd0aa4f31fa8b.png)\n",
      "\n",
      "## Chapter 2: Technical Performance\n",
      "\n",
      "## Self-Driving Cars\n",
      "\n",
      "Self-driving vehicles have long been a goal for AI researchers and technologists. However, their widespread adoption has been  slower  than  anticipated.  Despite  many  predictions that  fully  autonomous  driving  is  imminent,  widespread  use of  self-driving  vehicles  has yet  to  become  a  reality.  Still,  in recent  years,  signi fi cant  progress  has  been  made.  In  cities like  San  Francisco  and  Phoenix, fl eets  of  self-driving  taxis are  now  operating  commercially.  This  section  examines recent advancements in autonomous driving, focusing on deployment, technological breakthroughs and new benchmarks, safety performance, and policy challenges.\n",
      "\n",
      "## Deployment\n",
      "\n",
      "Self-driving cars ...\n",
      "\n",
      "========================================\n",
      "\n",
      "AI agents, autonomous or semiautonomous systems designed to operate within  speci fi c  environments to accomplish goals, represent an  exciting  frontier in  AI  research. These agents have a diverse range of potential  applications,  from  assisting in  academic research and scheduling meetings to facilitating online shopping  and  vacation  booking.  As suggested by many recent corporate releases,  agentic  AI  has  become  a topic of increasing interest in the technical world of AI.\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000006_60957158f1c4e3002ec6ab1ce22a95dd9fb64fde5d2273cea56877a94e3a7957.png)\n",
      "\n",
      "## 2.8 AI Agents\n",
      "\n",
      "For decades, the topic of AI agents has been widely discussed in the AI community, yet few benchmarks have achieved widespread adoption, including those featured in last year's Index, such as AgentBench and MLAgentBench. ...\n",
      "\n",
      "========================================\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000001_4f1f3d35cd439f1c0e9036eb691f1ec969156888b06e0456026e204461584042.png)\n",
      "\n",
      "## Chapter 2: Technical Performance\n",
      "\n",
      "2.2 Language\n",
      "\n",
      "The top model on the Berkeley Function Calling Leaderboard is  watt-tool-70b,  a fi ne-tuned  variant  of  Llama-3.3-70BInstruct designed speci fi cally for function calling. ...\n"
     ]
    }
   ],
   "source": [
    "response = chunks.query.hybrid(\n",
    "    query=\"Recent advances in RAG\",\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(o.properties[\"chunk\"][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a729583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:51.982344Z",
     "iopub.status.busy": "2025-09-04T16:17:51.982064Z",
     "iopub.status.idle": "2025-09-04T16:17:51.986003Z",
     "shell.execute_reply": "2025-09-04T16:17:51.985095Z",
     "shell.execute_reply.started": "2025-09-04T16:17:51.982324Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_image_paths(text):\n",
    "    \"\"\"Extract image paths from markdown-style image references.\"\"\"\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d036efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:52.555583Z",
     "iopub.status.busy": "2025-09-04T16:17:52.555303Z",
     "iopub.status.idle": "2025-09-04T16:17:52.559524Z",
     "shell.execute_reply": "2025-09-04T16:17:52.558780Z",
     "shell.execute_reply.started": "2025-09-04T16:17:52.555563Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_base64s(image_paths, base_path=None):\n",
    "    import base64\n",
    "    base64_images = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = Path(base_path) / img_path if base_path else Path(img_path)\n",
    "        image_bytes = full_path.read_bytes()\n",
    "        base64_string = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        base64_images.append(base64_string)\n",
    "\n",
    "    return base64_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "430397bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:17:53.151521Z",
     "iopub.status.busy": "2025-09-04T16:17:53.151239Z",
     "iopub.status.idle": "2025-09-04T16:17:53.160998Z",
     "shell.execute_reply": "2025-09-04T16:17:53.160202Z",
     "shell.execute_reply.started": "2025-09-04T16:17:53.151501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding image paths: []\n",
      "Adding image paths: []\n",
      "Adding image paths: []\n",
      "Adding image paths: []\n",
      "Adding image paths: ['data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000007_3921e74bcd236b403bab6328e8a11a403024116e502a0d0bbf5264f43935b867.png', 'data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000008_33bf09ad90ca2e078ea69ec3e2e6beeea2fb2395ebd182a5b6e68c8d215648de.png']\n",
      "Adding image paths: []\n",
      "Adding image paths: ['data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000009_8670c86bc1f4a1d41097f5ec286999521dc5bbf65615ebf4fb76a6a03b679acd.png']\n",
      "Adding image paths: ['data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000012_90c7c87f35758c0ce8c4a95f97089830098242f6ae4df85ab58cd0aa4f31fa8b.png']\n",
      "Adding image paths: ['data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000006_60957158f1c4e3002ec6ab1ce22a95dd9fb64fde5d2273cea56877a94e3a7957.png']\n",
      "Adding image paths: ['data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000001_4f1f3d35cd439f1c0e9036eb691f1ec969156888b06e0456026e204461584042.png']\n"
     ]
    }
   ],
   "source": [
    "all_chunks = \"\"\n",
    "all_images = []\n",
    "\n",
    "for o in response.objects:\n",
    "    chunk_text = o.properties[\"chunk\"]\n",
    "    image_paths = extract_image_paths(chunk_text)\n",
    "    print(f\"Adding image paths: {image_paths}\")\n",
    "    all_images.extend(get_image_base64s(image_paths, base_path=\"data/parsed\"))\n",
    "\n",
    "    all_chunks += \"\\n\\n\" + chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93da885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:19:59.520194Z",
     "iopub.status.busy": "2025-09-04T16:19:59.519766Z",
     "iopub.status.idle": "2025-09-04T16:19:59.524473Z",
     "shell.execute_reply": "2025-09-04T16:19:59.523664Z",
     "shell.execute_reply.started": "2025-09-04T16:19:59.520169Z"
    }
   },
   "outputs": [],
   "source": [
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": []\n",
    "    }\n",
    "]\n",
    "\n",
    "for img in all_images:\n",
    "    content = {\n",
    "        \"image\": {\n",
    "            \"format\": \"png\",\n",
    "            \"source\": {\n",
    "                \"bytes\": img\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    message_list[0][\"content\"].append(content)\n",
    "\n",
    "task_text = \"\"\"\n",
    "What does this text tell us about the latest developments in RAG?\n",
    "\n",
    "Describe the details from the figures as well, if necessary.\n",
    "\"\"\" + \"\\n\\n\" + all_chunks\n",
    "message_list[0][\"content\"].append({\"text\": task_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0c9d7-352c-4ea9-ba2a-8e0eac890adf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T16:20:08.746277Z",
     "iopub.status.busy": "2025-09-04T16:20:08.745986Z",
     "iopub.status.idle": "2025-09-04T16:20:13.468754Z",
     "shell.execute_reply": "2025-09-04T16:20:13.468212Z",
     "shell.execute_reply.started": "2025-09-04T16:20:08.746256Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/botocore/auth.py:425: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  datetime_now = datetime.datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Response Content Text]\n",
      "### Latest Developments in RAG\n",
      "\n",
      "The text highlights several key advancements and benchmarks in Retrieval-Augmented Generation (RAG):\n",
      "\n",
      "1. **RAG Systems**: RAG integrates LLMs with retrieval mechanisms to enhance response generation. It retrieves relevant information from files or documents and generates tailored responses based on the retrieved content. Use cases include answering precise questions from large databases and addressing customer queries using company documents.\n",
      "\n",
      "2. **Benchmarks**: \n",
      "   - **Ragnarok**: A RAG arena battleground.\n",
      "   - **CRAG**: Comprehensive RAG benchmark.\n",
      "   - **FinanceBench**: Specialized RAG benchmark for financial question answering.\n",
      "\n",
      "3. **Technical Innovations**:\n",
      "   - **Anthropic's Contextual Retrieval**: Introduced in September 2024, significantly enhances RAG's retrieval capabilities.\n",
      "\n",
      "### Figures and Details\n",
      "\n",
      "1. **Berkeley Function Calling Leaderboard**:\n",
      "   - Evaluates LLMs' ability to accurately call functions or tools.\n",
      "   - Includes over 2,000 question-function-answer pairs across multiple programming languages and testing domains.\n",
      "\n",
      "2. **MTEB: Massive Text Embedding Benchmark**:\n",
      "   - Introduced in late 2022 by Hugging Face and Cohere.\n",
      "   - Evaluates models on various embedding tasks, essential for RAG.\n",
      "\n",
      "3. **VisualAgentBench (VAB)**:\n",
      "   - Launched in 2024 to evaluate agentic AI.\n",
      "   - Tests agents across three\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-west-2\",\n",
    ")\n",
    "\n",
    "# MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [    {\n",
    "        \"text\": \"You are an expert. Read the provided text and content of these images and answer the questions thoughtfully but succinctly if possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"maxTokens\": 300, \"topP\": 0.1, \"topK\": 20, \"temperature\": 0.3}\n",
    "\n",
    "native_request = {\n",
    "    \"schemaVersion\": \"messages-v1\",\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "# Invoke the model and extract the response body.\n",
    "response = client.invoke_model(modelId=MODEL_ID, body=json.dumps(native_request))\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Print the text content for easy readability.\n",
    "content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
