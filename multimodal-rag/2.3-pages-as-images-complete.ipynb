{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0fd1e8",
   "metadata": {},
   "source": [
    "# Multimodal RAG with Weaviate\n",
    "\n",
    "Search and generate responses using both text and images. We'll use PDF pages from Reddit's S-1 filing as our multimodal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef165ec",
   "metadata": {},
   "source": [
    "![images/multimodal_rag.png](images/multimodal_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqq weaviate-client==v4.17.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22adaf4c-9e5b-4f4c-b770-1b80124a4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh credentials & load the Weaviate IP\n",
    "from helpers import update_creds\n",
    "\n",
    "AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_SESSION_TOKEN = update_creds()\n",
    "\n",
    "%store -r WEAVIATE_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df35311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import os\n",
    "\n",
    "client = weaviate.connect_to_local(\n",
    "    WEAVIATE_IP,\n",
    "    headers = {\n",
    "        \"X-AWS-Access-Key\": AWS_ACCESS_KEY,\n",
    "        \"X-AWS-Secret-Key\": AWS_SECRET_KEY,\n",
    "        \"X-AWS-Session-Token\": AWS_SESSION_TOKEN,\n",
    "    }\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be154e4b",
   "metadata": {},
   "source": [
    "Run the following if you need to delete an existing collection and start fresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections.delete(\"Pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c22328",
   "metadata": {},
   "source": [
    "### Data import\n",
    "\n",
    "Create a collection to store PDF pages as images with embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715c025",
   "metadata": {},
   "source": [
    "We'll set up a collection where each object is based on a page of a PDF document. \n",
    "\n",
    "The vector configuration is set up with: `Configure.MultiVectors.self_provided`\n",
    "\n",
    "This specifies that it is a multi-vector embedding model (`Configure.MultiVectors`), and that the vectors will be provided by the user (`self-provided`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Property, DataType, Configure, Tokenization\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Pages\",\n",
    "    properties=[\n",
    "        Property(\n",
    "            name=\"document_title\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"page_image\",\n",
    "            data_type=DataType.BLOB,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"filename\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.FIELD\n",
    "        ),\n",
    "    ],\n",
    "    vector_config=[\n",
    "        Configure.Vectors.multi2vec_aws(\n",
    "            name=\"page\",\n",
    "            image_fields=[\"page_image\"],\n",
    "            region=\"us-west-2\",\n",
    "            model=\"amazon.titan-embed-image-v1\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb1675",
   "metadata": {},
   "source": [
    "Now we can load the data into the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ded476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = client.collections.use(\"Pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4aab86",
   "metadata": {},
   "source": [
    "Load images into the collection with automatic embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8533b77-844e-4f18-b15d-326fa822dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pymupdf\n",
    "except ImportError:\n",
    "    %pip install -Uqq pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d65c3-7489-428b-91cb-93ccde9516e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python pdf_to_img.py hai*.pdf\n",
    "echo \"Images extracted from AI Report PDF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import base64\n",
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "\n",
    "img_files = sorted(Path(\"data/imgs\").glob(\"*.jpg\"))\n",
    "\n",
    "with pages.batch.fixed_size(batch_size=10) as batch:\n",
    "    for filepath in tqdm(img_files[:100]):\n",
    "        image = filepath.read_bytes()\n",
    "        base64_image = base64.b64encode(image).decode('utf-8')\n",
    "        obj = {\n",
    "            \"document_title\": \"HAI report\",\n",
    "            \"page_image\": base64_image,\n",
    "            \"filename\": filepath.name\n",
    "        }\n",
    "\n",
    "        # Add object to batch for import with (batch.add_object())\n",
    "        batch.add_object(\n",
    "            properties=obj,\n",
    "            uuid=generate_uuid5(filepath.name)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9909d1",
   "metadata": {},
   "source": [
    "### Queries\n",
    "\n",
    "Search through images using text queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96623cc",
   "metadata": {},
   "source": [
    "Find the most relevant pages using semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbaf22a",
   "metadata": {},
   "source": [
    "Since we have the embedding to query with (`query_embedding`), we can use the `near_vector` method to find the most relevant pages in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac1d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pages.query.near_text(\n",
    "    query=\"RAG\",\n",
    "    limit=2,\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    print(f\"Filename: {o.properties['filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38906b7",
   "metadata": {},
   "source": [
    "Display the retrieved images to see what was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa462e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_imgs(images_to_display):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(30, 40))\n",
    "\n",
    "    for i, img_path in enumerate(images_to_display):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "images = [\n",
    "    f\"data/imgs/\" + o.properties['filename'] for o in response.objects\n",
    "]\n",
    "display_imgs(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370fdc8-5388-4762-9f20-e39007b58acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pages.query.near_text(\n",
    "    query=\"self-driving cars\",\n",
    "    limit=2,\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    print(f\"Filename: {o.properties['filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7f497-9889-49b3-b4bc-bd983481c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    f\"data/imgs/\" + o.properties['filename'] for o in response.objects\n",
    "]\n",
    "display_imgs(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322f672",
   "metadata": {},
   "source": [
    "### Retrieval augmented generation (RAG)\n",
    "\n",
    "Generate text responses based on image content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbb6a7",
   "metadata": {},
   "source": [
    "Combine image retrieval with AI text generation for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.generate import GenerativeConfig, GenerativeParameters\n",
    "\n",
    "prompt = GenerativeParameters.grouped_task(\n",
    "    prompt=\"What does this say about self-driving cars?\",\n",
    "    image_properties=[\"page_image\"]  # Property containing images in Weaviate\n",
    ")\n",
    "\n",
    "gen_config_aws = GenerativeConfig.aws(\n",
    "    region=\"us-west-2\",\n",
    "    service=\"bedrock\",\n",
    "    model=\"us.amazon.nova-pro-v1:0\"\n",
    ")\n",
    "\n",
    "# We use `pages.generate` here to generate a response based on the retrieved pages.\n",
    "response = pages.generate.near_text(\n",
    "    query=\"self-driving cars\",\n",
    "    limit=2,\n",
    "    # These parameters are used to define the RAG task & model\n",
    "    grouped_task=prompt,\n",
    "    generative_provider=gen_config_aws\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1048129",
   "metadata": {},
   "source": [
    "And the results are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.generative.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deafd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.generate import GenerativeConfig, GenerativeParameters\n",
    "\n",
    "prompt = GenerativeParameters.grouped_task(\n",
    "    prompt=\"What do each of these pages describe?\",\n",
    "    image_properties=[\"page_image\"]  # Property containing images in Weaviate\n",
    ")\n",
    "\n",
    "# We use `pages.generate` here to generate a response based on the retrieved pages.\n",
    "response = pages.generate.near_text(\n",
    "    query=\"advances in RAG\",\n",
    "    limit=3,\n",
    "    # These parameters are used to define the RAG task & model\n",
    "    grouped_task=prompt,\n",
    "    generative_provider=gen_config_aws\n",
    ")\n",
    "\n",
    "print(response.generative.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d3def",
   "metadata": {},
   "source": [
    "### In-depth research & analysis\n",
    "\n",
    "Use multimodal RAG for detailed document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d12e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.generate import GenerativeConfig, GenerativeParameters\n",
    "\n",
    "prompt = GenerativeParameters.grouped_task(\n",
    "    prompt=\"What do these pages highlight about the recent advances in RAG?\",\n",
    "    image_properties=[\"page_image\"]  # Property containing images in Weaviate\n",
    ")\n",
    "\n",
    "# We use `pages.generate` here to generate a response based on the retrieved pages.\n",
    "response = pages.generate.near_text(\n",
    "    query=\"advances in RAG\",\n",
    "    limit=3,\n",
    "    # These parameters are used to define the RAG task & model\n",
    "    grouped_task=prompt,\n",
    "    generative_provider=gen_config_aws\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.generative.text)\n",
    "for o in response.objects:\n",
    "    print(o.properties[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dfb84",
   "metadata": {},
   "source": [
    "### Close the client\n",
    "\n",
    "Always close your connection when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c426fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a11724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "202509-aws-genai-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
